Code From: `transformers/src/transformers/generation/beam_search.py` 
Repo Link - https://github.com/huggingface/transformers



## Code Snippet:

```
    def process(
        self,
        input_ids: torch.LongTensor,
        next_scores: torch.FloatTensor,
        next_tokens: torch.LongTensor,
        next_indices: torch.LongTensor,
        pad_token_id: Optional[int] = None,
        eos_token_id: Optional[Union[int, List[int]]] = None,
        beam_indices: Optional[torch.LongTensor] = None,
        group_index: Optional[int] = 0,
    ) -> Dict[str, torch.Tensor]:
        cur_len = input_ids.shape[-1] + 1  # add up to the length which the next_scores is calculated on
        batch_size = len(self._beam_hyps) // self.num_beam_groups

        if not (batch_size == (input_ids.shape[0] // self.group_size)):
            if self.num_beam_groups > 1:
                raise ValueError(
                    f"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam "
                    f"size of {self.group_size} is expected by the beam scorer."
                )
            else:
                raise ValueError(
                    f"A beam size of {input_ids.shape[0]} is used as the input, but a beam size of "
                    f"{self.group_size} is expected by the beam scorer."
                )

        device = input_ids.device
        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)
        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)
        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)

        if isinstance(eos_token_id, int):
            eos_token_id = [eos_token_id]

        for batch_idx in range(batch_size):
            batch_group_idx = batch_idx * self.num_beam_groups + group_index
            if self._done[batch_group_idx]:
                if self.num_beams < len(self._beam_hyps[batch_group_idx]):
                    raise ValueError(f"Batch can only be done if at least {self.num_beams} beams have been generated")
                if eos_token_id is None or pad_token_id is None:
                    raise ValueError("Generated beams >= num_beams -> eos_token_id and pad_token have to be defined")
                # pad the batch
                next_beam_scores[batch_idx, :] = 0
                next_beam_tokens[batch_idx, :] = pad_token_id
                next_beam_indices[batch_idx, :] = 0
                continue

            # next tokens for this sentence
            beam_idx = 0
            for beam_token_rank, (next_token, next_score, next_index) in enumerate(
                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])
            ):
                batch_beam_idx = batch_idx * self.group_size + next_index
                # add to generated hypotheses if end of sentence
                if (eos_token_id is not None) and (next_token.item() in eos_token_id):
                    # if beam_token does not belong to top num_beams tokens, it should not be added
                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size
                    if is_beam_token_worse_than_top_num_beams:
                        continue
                    if beam_indices is not None:
                        beam_index = beam_indices[batch_beam_idx]
                        beam_index = beam_index + (batch_beam_idx,)
                    else:
                        beam_index = None

                    self._beam_hyps[batch_group_idx].add(
                        input_ids[batch_beam_idx].clone(),
                        next_score.item(),
                        beam_indices=beam_index,
                    )
                else:
                    # add next predicted token since it is not eos_token
                    next_beam_scores[batch_idx, beam_idx] = next_score
                    next_beam_tokens[batch_idx, beam_idx] = next_token
                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx
                    beam_idx += 1

                # once the beam for next step is full, don't add more tokens to it.
                if beam_idx == self.group_size:
                    break

            if beam_idx < self.group_size:
                raise ValueError(
                    f"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:"
                    f" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected."
                )

            # Check if we are done so that we can save a pad step if all(done)
            self._done[batch_group_idx] = self._done[batch_group_idx] or self._beam_hyps[batch_group_idx].is_done(
                next_scores[batch_idx].max().item(), cur_len
            )

        return UserDict(
            {
                "next_beam_scores": next_beam_scores.view(-1),
                "next_beam_tokens": next_beam_tokens.view(-1),
                "next_beam_indices": next_beam_indices.view(-1),
            }
        )
```

## Coding Assistant Command: 

`Explain Code`

## Response:

```
What?
The process function takes input tensors (input_ids, next_scores, next_tokens, next_indices) along with optional parameters (pad_token_id, eos_token_id, beam_indices, group_index) and performs beam search decoding. It generates the next set of beam search candidates based on the input tensors and returns the scores, tokens, and indices of the next beams.

How?
The function first calculates the current length of the input tokens and initializes tensors for the next beam scores, tokens, and indices. It then iterates over the batch size and processes each batch separately. For each batch, it checks if the beam search is already done and handles the padding and continuation of beams accordingly. It adds the next predicted tokens to the beam candidates and checks if any beam has reached the maximum number of beams or the end of sentence token. Finally, it checks if the beam search is done for the current batch and returns the next beam scores, tokens, and indices as a dictionary.

Coupling and Cohesion
The function is tightly coupled with the internal state of the class it belongs to (self._beam_hyps, self.num_beam_groups, self.num_beams, self.group_size, self._done). It relies on the input tensors and optional parameters to perform the beam search decoding. The function has high cohesion as it focuses on a single task of generating the next beam search candidates.

Single Responsibility Principle
The code follows the Single Responsibility Principle as it performs the specific task of beam search decoding. However, there are some pieces of code that could be extracted into separate functions for better readability and maintainability. For example, the logic for handling padding and continuation of beams could be extracted into a separate function.

Unusual Things
The function uses a mixture of camel case and snake case for variable names (batch_group_idx, batch_beam_idx, beam_token_rank).
The function uses a combination of indexing and slicing to access and manipulate tensors (next_beam_scores[batch_idx, :], next_beam_tokens[batch_idx, :]).
The function uses a custom UserDict class to return the next beam scores, tokens, and indices as a dictionary.
Highly Suspicious
The function raises ValueError exceptions in certain conditions, but it does not provide detailed error messages or handle exceptions gracefully.
```
