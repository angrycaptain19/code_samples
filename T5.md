Code From: `transformers/src/transformers/generation/beam_search.py` 
Repo Link - https://github.com/huggingface/transformers



## Code Snippet:

```
    def process(
        self,
        input_ids: torch.LongTensor,
        next_scores: torch.FloatTensor,
        next_tokens: torch.LongTensor,
        next_indices: torch.LongTensor,
        pad_token_id: Optional[int] = None,
        eos_token_id: Optional[Union[int, List[int]]] = None,
        beam_indices: Optional[torch.LongTensor] = None,
        group_index: Optional[int] = 0,
    ) -> Dict[str, torch.Tensor]:
        cur_len = input_ids.shape[-1] + 1  # add up to the length which the next_scores is calculated on
        batch_size = len(self._beam_hyps) // self.num_beam_groups

        if not (batch_size == (input_ids.shape[0] // self.group_size)):
            if self.num_beam_groups > 1:
                raise ValueError(
                    f"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam "
                    f"size of {self.group_size} is expected by the beam scorer."
                )
            else:
                raise ValueError(
                    f"A beam size of {input_ids.shape[0]} is used as the input, but a beam size of "
                    f"{self.group_size} is expected by the beam scorer."
                )

        device = input_ids.device
        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)
        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)
        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)

        if isinstance(eos_token_id, int):
            eos_token_id = [eos_token_id]

        for batch_idx in range(batch_size):
            batch_group_idx = batch_idx * self.num_beam_groups + group_index
            if self._done[batch_group_idx]:
                if self.num_beams < len(self._beam_hyps[batch_group_idx]):
                    raise ValueError(f"Batch can only be done if at least {self.num_beams} beams have been generated")
                if eos_token_id is None or pad_token_id is None:
                    raise ValueError("Generated beams >= num_beams -> eos_token_id and pad_token have to be defined")
                # pad the batch
                next_beam_scores[batch_idx, :] = 0
                next_beam_tokens[batch_idx, :] = pad_token_id
                next_beam_indices[batch_idx, :] = 0
                continue

            # next tokens for this sentence
            beam_idx = 0
            for beam_token_rank, (next_token, next_score, next_index) in enumerate(
                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])
            ):
                batch_beam_idx = batch_idx * self.group_size + next_index
                # add to generated hypotheses if end of sentence
                if (eos_token_id is not None) and (next_token.item() in eos_token_id):
                    # if beam_token does not belong to top num_beams tokens, it should not be added
                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size
                    if is_beam_token_worse_than_top_num_beams:
                        continue
                    if beam_indices is not None:
                        beam_index = beam_indices[batch_beam_idx]
                        beam_index = beam_index + (batch_beam_idx,)
                    else:
                        beam_index = None

                    self._beam_hyps[batch_group_idx].add(
                        input_ids[batch_beam_idx].clone(),
                        next_score.item(),
                        beam_indices=beam_index,
                    )
                else:
                    # add next predicted token since it is not eos_token
                    next_beam_scores[batch_idx, beam_idx] = next_score
                    next_beam_tokens[batch_idx, beam_idx] = next_token
                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx
                    beam_idx += 1

                # once the beam for next step is full, don't add more tokens to it.
                if beam_idx == self.group_size:
                    break

            if beam_idx < self.group_size:
                raise ValueError(
                    f"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:"
                    f" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected."
                )

            # Check if we are done so that we can save a pad step if all(done)
            self._done[batch_group_idx] = self._done[batch_group_idx] or self._beam_hyps[batch_group_idx].is_done(
                next_scores[batch_idx].max().item(), cur_len
            )

        return UserDict(
            {
                "next_beam_scores": next_beam_scores.view(-1),
                "next_beam_tokens": next_beam_tokens.view(-1),
                "next_beam_indices": next_beam_indices.view(-1),
            }
        )
```

## Coding Assistant Command: 

`Generate Unit Tests`

## Response:

```
import pytest
import torch
from typing import Dict, Optional, Union
from torch import Tensor
from transformers.generation.beam_search import BeamSearch

@pytest.fixture
def beam_search():
    # Create an instance of the BeamSearch class for testing
    return BeamSearch()

@pytest.mark.parametrize(
    "input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, expected_output",
    [
        # Test case 1: Basic input values
        (
            torch.tensor([[1, 2, 3], [4, 5, 6]]),
            torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]),
            torch.tensor([[7, 8, 9], [10, 11, 12]]),
            torch.tensor([[0, 1, 2], [3, 4, 5]]),
            0,
            1,
            None,
            0,
            {
                "next_beam_scores": torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
                "next_beam_tokens": torch.tensor([0, 0, 0, 0, 0, 0]),
                "next_beam_indices": torch.tensor([0, 0, 0, 0, 0, 0]),
            },
        ),
        # Test case 2: Beam search done
        (
            torch.tensor([[1, 2, 3], [4, 5, 6]]),
            torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]),
            torch.tensor([[7, 8, 9], [10, 11, 12]]),
            torch.tensor([[0, 1, 2], [3, 4, 5]]),
            0,
            1,
            None,
            0,
            {
                "next_beam_scores": torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
                "next_beam_tokens": torch.tensor([0, 0, 0, 0, 0, 0]),
                "next_beam_indices": torch.tensor([0, 0, 0, 0, 0, 0]),
            },
        ),
        # Test case 3: Invalid input size
        (
            torch.tensor([[1, 2, 3], [4, 5, 6]]),
            torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]),
            torch.tensor([[7, 8, 9], [10, 11, 12]]),
            torch.tensor([[0, 1, 2], [3, 4, 5]]),
            0,
            1,
            None,
            0,
            {
                "next_beam_scores": torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
                "next_beam_tokens": torch.tensor([0, 0, 0, 0, 0, 0]),
                "next_beam_indices": torch.tensor([0, 0, 0, 0, 0, 0]),
            },
        ),
    ],
)
def test_process(
    beam_search,
    input_ids: Tensor,
    next_scores: Tensor,
    next_tokens: Tensor,
    next_indices: Tensor,
    pad_token_id: Optional[int],
    eos_token_id: Optional[Union[int, List[int]]],
    beam_indices: Optional[Tensor],
    group_index: Optional[int],
    expected_output: Dict[str, Tensor],
):
    # Arrange

    # Act
    output = beam_search.process(
        input_ids,
        next_scores,
        next_tokens,
        next_indices,
        pad_token_id,
        eos_token_id,
        beam_indices,
        group_index,
    )

    # Assert
    assert output == expected_output
```
